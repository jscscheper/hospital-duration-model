---
title: "Research log"
author: "Dennis Scheper"
date: "`r Sys.Date()`"
output:
  pdf_document:
    number_sections: yes
  sansfont: Arial
  html_document:
    df_print: paged
header-includes:
- \usepackage{booktabs}
---
\fontfamily{arial}
\fontsize{12}{15}
\selectfont

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(tidy=TRUE, message=FALSE, warning=FALSE, fig.align='center')
```

# Original dataset
Date: 11 September, 2020 (week 1)

## Introduction
The article, "Impact of HbA1c Measurement on Hospital Readmission Rates: Analysis of 70,000 Clinical Database Patient Records" states that hyperglycemia management has a significant impact on the outcome and readmission rates of hospitalized patients. The authors based this conclusion on the comprehensive assessment of 70.000 diabetes patient records retrieved from 140 US hospitals. The results that depict the relationship between readmission rates and the measurement of HbA1c levels can further enlargement of already existing diabetes tactics to reduce readmission rates.

## Dataset and Attributes Information
All information used in this article comes from a database, consisting of 41 tables and totals 117 features, such as demographics (like gender, race, and age), inpatient or outpatient, and (in-hospital) mortality. Data came from 130 hospitals in the USA for over ten years (1998-2008) and contained around 74 million unique visits by 18 million unique patients. This research used information that needs to accede to the following specifications: 
\begin{enumerate}
  \item Is a hospital admission; 
  \item The encounter is a diagnosed with 'diabetes', any kind will satisfy; 
  \item The length of admission was at least one day up to eighteen days; 
  \item Laboratory test results are available; and 
  \item Medications were administered. 
\end{enumerate}  
Of these five criteria points, 101.000 encounters fulfilled all specifications. After some considerations with removing encounters based on incomplete (weight and medical specialty) or biased data (discharge to a hospice or death), 69.984 encounters remained in the final dataset. 

The initial dataset consists of 55 attributes, with the class attribute being an encounter of one patient. As there are way too many attributes to describe, please refer to the codebook for all descriptions; we only look at some essential attributes and their type and possible valuations. The age of a patient is nominal and is grouped into ten-year intervals. The admission type or for what specific reason a patient was hospitalized and comes in 9 distinct values while the type is nominal. Some attributes are numeric and count, such as the number of lab procedures, the number of medications, and the number of emergency visits. The database consists of three diagnosis attributes, which can have 848, 923, and 954 distinct values, respectively. The values are based on ICD9 three-letter codes and are of the nominal type. Other vital attributes are whether a patient changed medications (with the values 'no change' and 'change'; nominal type) or had diabetes medication ('yes' or 'no' values and nominal typing). Twenty-four other attributes depict whether medicine is prescribed or not, if prescribed, then if the dosage was increased ('up'), decreased ('down'), or stayed the same ('steady') during the encounter. Readmission rates were calculated by looking at a nominal type ('Readmitted') with the possible valuations of '<30' for a patient that was readmitted within 30 days, '>30' for a patient that was readmitted after 30 days, and 'No' for patients that were not readmitted. The authors' goal was to determine whether a relationship between readmission rates and HbA1c measurement exists; therefore, they introduced a new attribute 'HbA1c' with four different valuations, based on the information from the database: 1) no HbA1c test performed; 2) HbA1c performed and in the normal range; 3) HbA1c performed and the result is greater than 8% with no change in diabetic medication; and 4) Hb1Ac performed, the result is more significant than 8%, and diabetic medication was changed.

## Research Question
Is it possible, using machine learning techniques, to predict a patient's time in hospital  linked to the results of an HbA1c measurement?

\newpage
# EDA (Exploratory Data Analysis)
## Date: 14 September, 2020 
This section will perform an exploratory data analysis (EDA) on the dataset described above. With an EDA, we can explore our dataset and determine if any correlations exist between attributes and undermine any missing values. If any exist, we deal with them accordingly, looking to repair these values or remove them entirely. Additionally, we will look at variations between and in attributes for determining which ones are of most importance and interest to our research goals. 

First, we load in our used packages, mostly used for visualization of data, and the data itself. Besides that, we also load in a codebook, which contains, i.e., a description for every attribute for the initial dataset. The codebook was not retrieved from any outside source but was constructed on the interpretation of the data.

```{r Load used packages and read in some important files}
# Load used packages
library(formattable)
library(plyr)
library(dplyr, warn.conflicts = FALSE)
library(tidyr)
library(tidyverse)
library(ggplot2)
library(grid)
library(gridExtra)
library(hexbin)
library(foreign)
library(xtable)

# Load in used data
encounter.data <- read.table(file = 'datasets/data_diabetes_retrieved/diabetic_data.csv',
                                              sep = ',', header = TRUE)

codebook <- read.csv(file = 'misc/codebook.csv', sep = ';',
                       header = T)
```

To get a better picture of our dataset's distribution, we called upon the function glimpse(). We notice that the data contains `r ncol(encounter.data)` columns/ attributes. Additionally, a summary() function gives an outline of every attribute, showing each level with a count of its valuation.

```{r Summarize data}
# Give a glimpse of how the data is distributed
glimpse(encounter.data)
# Give a quick summary of the data, and give information such as min and max value, median and mean
summary(encounter.data)
```
\newpage
## Missing values
As we see in the result determined by the glimpse() function, the columns race, weight, payer_code, gender, diag_3, and medical specialty have some or significant amounts of missing values. To get a better picture, we will zoom in on these attributes and determine the amount and the percentage of missing values.

```{r Determine missing values}
myvars <- c('race', 'weight', 'payer_code', 'medical_specialty', 'gender', 'diag_3')
amountRecords <- length(rownames(encounter.data))
tableMissingValues <- encounter.data %>%
  summarise_each_(funs("TotalMissing" = sum(. %in% c('?', 'Unknown/Invalid'), na.rm = TRUE), 
                       "MissingValuesPercentage" = round(sum(. %in% c('?', 'Unknown/Invalid'), na.rm = TRUE)/amountRecords * 100, 2)), myvars) %>%
  t()
tableMissingValues <- data.frame("Missing.Values" = tableMissingValues[1:6,], 
                                 "Missing.Values.Percentage" = tableMissingValues[7:12,])
rownames(tableMissingValues) <- myvars

# Added 27-09
library(naniar)
encounter.data.missing <- replace(encounter.data, encounter.data == "?", NA)
vis_miss(encounter.data.missing, warn_large_data = F)
```

[INSERT THE NEW TABLE ABOUT MISSING VALUES]

As we notice in table 1, attribute weight is almost entirely made-up of missing values. For that reason, it is a candidate for removal. The same can be said for payer_code and medical_specialty, where `r tableMissingValues['payer_code', 2]`% and `r tableMissingValues['medical_specialty', 2]`% of values are missing, respectively. Payer_code has no significant value to our research goals and questions; therefore, it can be removed. Medical_specialty can be of value as it contains a range of useful information. Missing values can be set to 'Missing' or reevaluated based on other valuations; doing this is not without risk, as it is almost half of the attributes' values. Deleting all missing values is not doable as it removes half of all records! Setting it to 'Missing' seems to be the most logical option. Attributes race and diag_3 seem to have little amounts of missing data, but removing these values can alter our outcomes; therefore, we will use the same approach as to medical_specialty. Gender only has three missing values. Considering the amount does not account to even one percent, removal does not seem to harm, and gender is, in most cases, considered binary data.

In the next section, we will introduce a new attribute (HbA1c measurement) based on an A1C test and the response to that result, which is defined as a change in diabetic medication. This test was performed at the time of hospital admission. We consider four groups of encounters:
\begin{enumerate}
  \item no HbA1c test performed;
  \item HbA1c performed and in normal range;
  \item HbA1c performed and the result is greater than 8% with no change in diabetic medication; and
  \item HbA1c performed, result is greater than 8%, and diabetic mediation was changed.
\end{enumerate}

```{r Introducing a new attribute}
encounter.data <- encounter.data %>%
  mutate(hba1c_res=ifelse((A1Cresult=='None'), 1, NA)) %>%
  mutate(hba1c_res=ifelse((A1Cresult %in% c('Norm', '>7')) & is.na(hba1c_res), 2, hba1c_res)) %>%
  mutate(hba1c_res=ifelse((A1Cresult=='>8') & (change=='No') & is.na(hba1c_res), 3, hba1c_res)) %>%
  mutate(hba1c_res=ifelse((A1Cresult=='>8') & (change=='Ch') & is.na(hba1c_res), 4, hba1c_res))
encounter.data$hba1c_res <- as.factor(encounter.data$hba1c_res)
```

# Added 24-10
We have a lot of near-zero variance variables comprised in our dataset, for example the attribute examide consists of entirely one level with no missing values. For that reason, it is a zero-variance attribute. This attribute is not informative and will not help in predicting an outcome. All other (near) zero-variance attributes were removed as they would increase computing times if kept. We are talking about the following eighteen attributes:

```{r}
library(caret)
removal.names <- nearZeroVar(encounter.data, names = T)
encounter.data <- encounter.data %>%
  select(-c(removal.names))
```

(Date: 18-20 September, 2020)

Before we start analyzing our dataset, it is crucial to check on duplicates as our dataset contains multiple inpatient visits for some patients. These observations cannot be statistically independent and would create noise. We thus declare that only one encounter per patient is optimal.

```{r Finding duplicates}
detach("package:plyr", unload = TRUE)
duplicateCheck <- encounter.data %>%
  group_by(patient_nbr) %>%
  summarise(count = n()) %>%
  filter(count > 1)
table.dup <- data.frame('Initial number of records' = nrow(encounter.data),
                        'Number of Duplicates' = nrow(duplicateCheck),
                        'Difference in percent' = round((nrow(encounter.data) - nrow(duplicateCheck) - nrow(encounter.data)) / nrow(encounter.data) * 100, 2) )
```
\begin{table}[ht]
\centering
\begin{tabular}{rrrr}
  \hline
 Initial number of records & Number of duplicates & Difference in percent \\ \hline
 101766 & 16773 & -16.48 \\ \hline
\end{tabular}
\end{table}

The initial dataset started with `r table.dup['Initial.number.of.records', 1]` records, of which `r table.dup['Number.of.Duplicates', 1]` were duplicates. A potential removal of these records would leave us with `r nrow(encounter.data) - nrow(duplicateCheck)` records, a total loss of `r table.dup['Difference.in.percent', 1]`%. A small price to pay for a statistically independent dataset.

\newpage
## Categorial attributes
In this section, we take a look at variations in and between attributes. The attribute gender consists of three values ('female,' 'male,' and 'missing/unknown'). Since observe and use this attribute, it is essential to remove the abundant value.

```{r Show -basic- variations}
# First we need to remove all records containing the third option of gender: 'Missing/unknown',
# as this can be lethal in further analysis
encounter.data <- encounter.data %>%
  select(everything()) %>%
  filter(gender != 'Unknown/Invalid') %>%
  droplevels()

# Count specific variables of interest
agg <- count(encounter.data, age, gender, A1Cresult, race)

# Specify a color per value for visualization
ecols <- c(Female = "pink", Male = "blue2")
p1 <- ggplot(agg) +
       geom_col(aes(x = race, y = n, fill = gender)) +
       scale_fill_manual(values = ecols) +
       labs(title="Race distributed with gender",
            x ="Race", y = "Total counts", subtitle = "1(a)")
p2 <- ggplot(agg) +
       geom_col(aes(x = age, y = n, fill = gender)) +
       scale_fill_manual(values = ecols) +
       labs(title="Age distributed with gender",
            x ="Age", y = "Total counts", subtitle = "1(b)")

ecols <- c(">7" = "blue", ">8" = "orange", "None" = "green", "Norm" = "yellow")
p3 <- ggplot(agg) +
       geom_col(aes(x = age, y = n, fill = A1Cresult)) +
       scale_fill_manual(values = ecols) +
       labs(title="Age distributed with A1C test result",
            x ="Age", y = "Total counts", subtitle = '2')
      

# Arrange p1 and p2 in a multiplot and leave p3 on its own
grid.arrange(p1, p2)
grid.arrange(p3)
```
Looking at figure 1a, we notice that most patients are from the Caucasian race with almost equivalent male and female ratios. We expect a rise in the number of diabetes patients when looking at older population groups. Figure 1b shows this exact prediction; the data is negatively skewed and increases in numbers per older age group. We observe the same results in figure 2, where an increase in A1C test is shown when comparing older population groups. Additionally, the figure gives an essential insight for many patients - in most cases, the test was not conducted and shows that strategies surrounding testing diabetes are not normalized in hospital protocols. 

This graph does not, however, make a distinction between non-ICU and ICU patients. The authors of the original dataset analyses stated that ICU departments' protocols have a stricter policy surrounding testing for diabetes. When comparing non-ICU and ICU patient records, we need to construct a new attribute called 'icu_or_non,' comprising data from attributes admission_type_id, admission_source_id, and discharge_disposition_id. We distinguish between non-ICU and ICU patients.

```{r ICU versus non-ICU}
encounter.data <- encounter.data %>%
  # Removing dead patients
  filter(discharge_disposition_id != 11) %>%
  # Distinction between ICU and non-ICU patients
  mutate(admission_type_id = ifelse(admission_type_id %in% c(1, 2, 7),
                                    'ICU patient', 'Non-ICU patient')) %>%
  mutate(admission_source_id = ifelse(admission_source_id %in% c(4, 7, 10, 12, 26),
                                      'ICU patient', 'Non-ICU patient')) %>%
  mutate(discharge_disposition_id = ifelse(discharge_disposition_id %in% c(13, 14, 19, 20, 21),
                                           'ICU patient', 'Non-ICU patient'))
# Now that we changed the labels, let us discover how many ICU and non-ICU patients we have
encounter.data <- encounter.data %>%
  # All columns need to be equal to each other
  mutate(icu_or_non = ifelse(admission_source_id == discharge_disposition_id &
                             admission_type_id   == discharge_disposition_id,
                             admission_source_id, 
                             ifelse(admission_source_id == discharge_disposition_id,
                                    admission_source_id, admission_source_id)))
encounter.data$icu_or_non <- as.factor(encounter.data$icu_or_non)

agg <- count(encounter.data, gender, A1Cresult, icu_or_non, race)
ecols <- c(Female = "pink", Male = "blue2")
p1 <- ggplot(agg) +
       geom_col(aes(x = icu_or_non, y = n, fill = gender)) +
       scale_fill_manual(values = ecols) +
       labs(title="ICU statistics distributed with gender",
            x ="ICU or non-ICU patient", y = "Total counts",
            subtitle = "3(a)")
       
p2 <- ggplot(agg) +
       geom_col(aes(x = icu_or_non, y = n, fill = race)) +
       labs(title="ICU statistics distributed with race",
            x ="ICU or non-ICU patient", y = "Total counts",
            subtitle = "3(b)")
       
ecols <- c('ICU patient' = 'blue', 'Non-ICU patient' = 'red')
p3 <- ggplot(agg) + 
       geom_col(aes(x = A1Cresult, y = n, fill = icu_or_non)) +
       scale_fill_manual(values = ecols) +
       labs(title="ICU statistics distributed with A1C test result",
            x ="ICU or non-ICU patient", y = "Total counts", subtitle = '4')

grid.arrange(p1, p2)
grid.arrange(p3)
```
We observe the result from the distinction between ICU and non-ICU in figures 3a, 3b, and 4. We notice a smaller population of non-ICU patients in all mentioned figures, whereas the ratio of not only gender but also seen in 3b, where race is depicted, ratios stay consequently the same. Figure 4 shows that no matter the patient type (ICU or non-ICU), protocols surrounding diabetes testing is not firmly conducted. Keeping the distinction between non-ICU and ICU might not be necessary. However, it can be of complimentary use when starting with machine learning; the data is binary, and it allows the removal of three attributes.

The attributes diag_1, diag_2, and diag_3 consist of many three-digit ICD codes. Many of these codes belong together in a subgroup. In the following section, we will construct a better way of describing the exact diagnosis. ICD codes descriptions are retrieved from \cite{ICD}.

```{r Diagnosis}
encounter.data <- encounter.data %>%
  mutate(diag_1 = case_when(diag_1 %in% c(390:459) ~ 'Circulatory',
                            diag_1 %in% c(460:519) ~ 'Respiratory',
                            diag_1 %in% c(520:579) ~ 'Digestive',
                            diag_1 %in% c(240:279) ~ 'Diabetes',
                            diag_1 %in% c(800:999) ~ 'Injury',
                            diag_1 %in% c(710:739) ~ 'Muscoloskeletal',
                            diag_1 %in% c(580:629) ~ 'Genitourinary',
                            TRUE ~ 'Others'))

encounter.data <- encounter.data %>%
  mutate(diag_2 = case_when(diag_2 %in% c(390:459) ~ 'Circulatory',
                            diag_2 %in% c(460:519) ~ 'Respiratory',
                            diag_2 %in% c(520:579) ~ 'Digestive',
                            diag_2 %in% c(240:279) ~ 'Diabetes',
                            diag_2 %in% c(800:999) ~ 'Injury',
                            diag_2 %in% c(710:739) ~ 'Muscoloskeletal',
                            diag_2 %in% c(580:629) ~ 'Genitourinary',
                            TRUE ~ 'Others'))
encounter.data <- encounter.data %>%
  mutate(diag_3 = case_when(diag_3 %in% c(390:459) ~ 'Circulatory',
                            diag_3 %in% c(460:519) ~ 'Respiratory',
                            diag_3 %in% c(520:579) ~ 'Digestive',
                            diag_3 %in% c(240:279) ~ 'Diabetes',
                            diag_3 %in% c(800:999) ~ 'Injury',
                            diag_3 %in% c(710:739) ~ 'Muscoloskeletal',
                            diag_3 %in% c(580:629) ~ 'Genitourinary',
                            TRUE ~ 'Others'))

# Convert to factors again
cols <- c('diag_1', 'diag_2', 'diag_3')
encounter.data[cols] <- lapply(encounter.data[cols], factor)
agg <- count(encounter.data, gender, diag_1, diag_2, diag_3)

# Specify a color per value for visualization
ecols <- c(Female = "pink", Male = "blue2")
p1 <- ggplot(agg) +
       geom_col(aes(x = diag_1, y = n, fill = gender)) +
       scale_fill_manual(values = ecols) +
      labs(title="Primary diagnosis distributed with gender",
            x ="Primary diagnosis [category]", y = "Total counts",
            subtitle = "5(a)")
p2 <- ggplot(agg) +
       geom_col(aes(x = diag_2, y = n, fill = gender)) +
       scale_fill_manual(values = ecols) +
       labs(title="Secondary diagnosis distributed with gender",
            x ="Secondary diagnosis [category]", y = "Total counts",
            subtitle = "5(b)")
p3 <- ggplot(agg) +
       geom_col(aes(x = diag_3, y = n, fill = gender)) +
       scale_fill_manual(values = ecols) +
       labs(title="Final diagnosis distributed with gender",
            x ="Final diagnosis [category]", y = "Total counts",
            subtitle = "5(c)")

grid.arrange(p1, p2, p3)
```

Looking at figure 5, we can see the results of the revaluation of attributes diag_1, diag_2, and diag_3. These attributes depict the primary, secondary, and final diagnosis of a patient, respectively. Interestingly, the difference between the figures is the increase in the number of diabetes diagnoses. Due to not testing diabetes on the initial hospitalization, readmission rates increase as a patient's primary diagnosis is not sustainable. The data classification now shows a clearer picture and would undoubtedly be of fair use in the final dataset. The original authors did not keep diag_2 and diag_3, as it would make records too complex to achieve their goals. Removal of these two attributes is still up for discussion, but the analysis does not give - for now - a clear indication for potential removal.

In the next section, we look at attribute medical_specialty and decide whether it is useful enough - considering the number of missing values - to be a candidate for the final dataset. We construct multiple plots that zoom in on the categories and valuations of this attribute.

```{r Medical specialty}
library(pals)

# Count data we want to compare
agg <- count(encounter.data, age, medical_specialty, gender)

# Determine a color scheme for 71 values
ecols <- c(alphabet(26), cols25(25), glasbey(22))

# Order variables from high to low via '-n'
agg_ord <- mutate(agg,
                  age = reorder(age, -n, sum),
                  medical_specialty = reorder(medical_specialty, -n, sum))
# p1: bar plot combined, and p2: per gender
p1 <- ggplot(agg_ord) +
      geom_col(aes(x = age, y = n, fill = medical_specialty)) +
      scale_fill_manual(values = ecols) +
      theme(legend.position = 'none') +
      labs(title="Medical specialty distributed with age groups and divided into gender",
           x ="Age [group]", y = "Total counts", subtitle = '6')
      
p1 <- p1 + facet_wrap(~gender) +
      theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
# Do the same as above, only now with pie charts
p2 <- ggplot(agg_ord) +
      geom_col(aes(x = 1, y = n, fill = medical_specialty), position = "fill") +
      coord_polar(theta = "y") +
      scale_fill_manual(values = ecols) +
      theme(legend.position = 'none')
p2 <- p2 + facet_wrap(~gender) +
    theme_bw() +
    theme(legend.position = 'none')


# Plot all results
grid.arrange(p1)
grid.arrange(p2)
```
Looking at both figures 6 and 7, we observe a large amount of valuation between all demographic attributes used. Depicted in purple are the missing values, as already established, make-up almost 50 percent of total records. Medical specialty is different from, for example, admission_id, where we could shrink the attribute to a better format. This is an alternative to this attribute. We are considering this with the fact that medical specialty does not give better information than any diagnosis attribute, which also gives, maybe even more useful, guidance about an encounter's medical history. At this stage, removal of this attribute could not be of lethal harm. 

For our final categorical attribute, and one of the most influential -according to the original authors- we will discuss readmitted. Readmitted is an attribute with three different valuations: '<30' for readmission within 30 days after release, '>30' for readmission after 30 days release, and 'NO' for no readmission. 

```{r Readmission}
agg <- count(encounter.data, readmitted, race, gender, age, diag_1, diag_2)
ecols <- c('<30' = 'red', '>30' = 'blue', 'NO' = 'pink')

p1 <- ggplot(agg) +
       geom_col(aes(x = race, y = n, fill = readmitted)) +
       scale_fill_manual(values = ecols) +
       labs(title="Readmitted distributed with race and divided into gender",
            x ="Race [group]", y = "Total counts", subtitle = "8(a)")
p1 <- p1 + facet_wrap(~gender) +
       theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
p2 <- ggplot(agg) +
       geom_col(aes(x = age, y = n, fill = readmitted)) +
       scale_fill_manual(values = ecols) +
       labs(title="Readmitted distributed with age and divided into gender",
            x ="Age [group]", y = "Total counts", subtitle = "8(b)")
p2 <- p2 + facet_wrap(~gender) + 
       theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
p3 <- ggplot(agg) +
       geom_col(aes(x = diag_1, y = n, fill = readmitted)) +
       scale_fill_manual(values = ecols) +
       labs(title="Readmitted distributed with primary diagnosis and divided into gender",
            x ="Primary diagnosis [category]", y = "Total counts", subtitle = '9')
p3 <- p3 + facet_wrap(~gender) +
       theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
p4 <- ggplot(agg) +
       geom_col(aes(x = diag_2, y = n, fill = readmitted)) +
       scale_fill_manual(values = ecols) +
       labs(title="Readmitted distributed with secondary diagnosis and divided into gender",
            x ="Secondary diagnosis [category]", y = "Total counts", subtitle = '10')
p4 <- p4 + facet_wrap(~gender) +
       theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

grid.arrange(p1, p2)
grid.arrange(p3)
grid.arrange(p4)
```
We observe the results of analyzing the readmitted attribute. We looked at readmitted with different demographics (gender, age, and race) and two of the three diagnosis attributes. Figure 8a shows that the Caucasian race has the most readmission rates in both genders. Observations made in figure 8b depict that as age increases, readmission rates also increase for females and males. This makes sense as immunity decreases with age, and the chance of recovery diminishes. Readmission rates decrease after the age of 80. The reasoning behind this might be death or transferring to a hospice or other facility. 

Comparing the results in figures 9 and 10, we see a familiar trend: the increase of diabetes diagnoses between the primary and secondary diagnoses. This typically means that patients admitted were diagnosed differently, and over time, are getting a new diabetes diagnosis much quicker. 

Seeing as we have multiple diagnosis categories but are somewhat interested in diabetes only, we can alter these attributes to have only two valuations ('diabetes' and 'other'). However, doing this can affect later analysis as we remove potentially valuable information. 

## Distribution - numeric attributes
We discussed many categorical data attributes, now turn to our few numeric data. It is important to have a good distribution between numeric data. If the range of distribution is too large, then normalization is necessary. We construct multiple histograms, which we get through the histogram.plotter() function.

```{r Distribution}
# Define a histogram plotter to construct multiple figures really quick
histogram.plotter <- function(attribute_1, number, attribute_name){
  histogram <- ggplot() +
                geom_bar(mapping = aes(x=attribute_1)) +
                ggtitle(number) +
                xlab(attribute_name)
  print(histogram)
}

# Make a smaller dataset so that only the numeric data is collected
numeric.data <- select_if(encounter.data, is.numeric)
# Remove encounter and patient ID
numeric.data <- numeric.data[, -c(1, 2)]
smaller.codebook <- codebook[c(10, 13:18, 22), 2]
numeric.data.log <- log2(numeric.data + 0.1)

wrapper <- function(data){
  p <- list()
  for ( i in c(1:length(data)) ){
    p[[i]] <- histogram.plotter(data[, i], LETTERS[i], smaller.codebook[i])
  }
  return(p)
}
numeric.normal <- wrapper(numeric.data)
numeric.log <- wrapper(numeric.data.log)
# Arrange in two multiplots
do.call(grid.arrange, numeric.normal[1:8])
ggsave(filename = "figures/distribution_his_1.pdf", width = 10, height = 8, plot = do.call(grid.arrange, numeric.normal[1:8]))
do.call(grid.arrange, numeric.log[1:8])
ggsave(filename = "figures/distribution_his_2.pdf", width = 10, height = 8, plot = do.call(grid.arrange, numeric.log[1:8]))
```
Looking at the results, depicted in figures 11 and 12, shows that not all of the attributes are evenly distributed. Normalization might be necessary to correct the distributions of namely B, which makes a couple of weird spikes, E, F, and G, which are all -but B- number of visits of an encounter. We might want to introduce a new attribute, total_visits, which accounts for all visits made by one encounter. The method of normalization is still up for debate.

\newpage
## Correlation - numeric attributes
To discover any correlation between the numeric data in our dataset, we constructed a heatmap, which is depicted below. We discuss a possible correlation between age and time spent in the hospital and the potential relationship between the num_lab_procedures and num_medications.
```{r Heatmaps}
library(reshape2)
melted.data <- numeric.data %>%
  cor() %>%
  melt()

# Construct the heatmap
ggplot(data = melted.data, aes(x = Var1, y = Var2, fill = value)) +
geom_tile() + 
  scale_fill_gradient2(low = "orange", high = "red", mid = "yellow") +
  theme(axis.text.x = element_text(angle = 90))
ggsave(filename = "figures/heatmap.pdf", plot = last_plot(), width = 10, height = 8)

# Is the amount of time spent in hospital age-dependent?
encounter.data %>%
  count(age, time_in_hospital) %>%
  ggplot(mapping = aes(x = age, y = time_in_hospital)) +
    geom_tile(mapping = aes(fill = n)) +
    labs(subtitle = '14')

# Is there a correlation between num_lab_procedures and num_medications?
encounter.data %>%
  ggplot() +
   geom_hex(mapping=aes(x = num_lab_procedures, y = num_medications)) +
   labs(subtitle = '15')
```

Looking at our heatmap (shown in figure 13), we determine a strong correlation as red, a small correlation as orange, and no correlation as yellow. The attributes that stand out are, for example, num_medications-time_in_hospital, num_procedures-num_medications, and hba1c_res-num_medications. As we can see, it is mostly the num_medication attribute that has some correlation. Others do not stand out that much. We can conclude that most of the attributes are non-correlated, at least for the numeric attributes. 

Looking at figure 14, we observe that the age groups above [40-50) increases in total counts; consequently, the total time spent in the hospital before release also increases. We can, therefore, conclude that the time_in_hospital attribute is dependent on a patient's age. Finally, we zoom in on two attributes that had a (small) correlation depicted in figure 13. Figure 15 shows this the result of zooming in on num_medications and num_lab_procedures, the result shows that most encounters have a valuation between 15-20 medications and 40-60 amount of lab procedures. The result also shows a lot of outliers, which normalization could solve.

```{r PCA}
cc.pca <- prcomp(numeric.data,
                 center = TRUE,
                 scale. = TRUE)
plot(cc.pca, type = 'l', main = "Correlation PCA")

library(ggbiplot)
ggbiplot(cc.pca, obs.scale = 1, var.scale = 1, groups = as.factor(encounter.data$time_in_hospital),
         ellipse = TRUE, circle = TRUE, alpha = 0.5) +
  scale_color_discrete(name = '') +
  theme(legend.direction = 'horizontal', legend.position = 'top') +
  xlim(-5, 20) + ylim(-10, 50)
ggsave(filename = 'figures/pca-plot.pdf', plot = last_plot(), width = 10, height = 8, dpi = 300)
```


\newpage
# A Clean Dataset
## 28th of September
In this section, we will remove all unnecessary attributes and instances as discussed in the EDA. Many filters and mutations have already been performed throughout the EDA, but not all have made it and seems to be scattered around - to give a more clear picture of the final, clean dataset, we perform all filtering, mutations, and relabeling that made it to the final dataset, again. 

```{r Restructering and removal of abdundant data}
# Load in the data again to perform only filtering that is needed, discussed in the EDA
final.data <- read.table(file = 'datasets/data_diabetes_retrieved/diabetic_data.csv',
                         sep = ',', header = TRUE)

final.data <- final.data %>%
  # Filter out every duplicate, based on patient number
  distinct(patient_nbr, .keep_all = TRUE) %>%
  # Relabel '?' to 'Missing'
  mutate(race=recode(race,'?'='Missing')) %>%
  # Remove instances with a missing labels and patients whom died in hospital
  filter(gender != 'Unknown/Invalid' & discharge_disposition_id != 11) %>%
  # Introduce the 'hba1c_res attribute, which is our class attribute
  mutate(hba1c_res=ifelse((A1Cresult=='None'), '1', NA)) %>%
  mutate(hba1c_res=ifelse((A1Cresult %in% c('Norm', '>7')) & is.na(hba1c_res), '2', hba1c_res)) %>%
  mutate(hba1c_res=ifelse((A1Cresult=='>8') & (change=='No') & is.na(hba1c_res), '3', hba1c_res)) %>%
  mutate(hba1c_res=ifelse((A1Cresult=='>8') & (change=='Ch') & is.na(hba1c_res), '4', hba1c_res)) %>%
  # Introduce the new attribute icu_or_non
  mutate(admission_type_id = ifelse(admission_type_id %in% c(1, 2, 7),
                                    'ICU patient', 'Non-ICU patient')) %>%
  mutate(admission_source_id = ifelse(admission_source_id %in% c(4, 7, 10, 12, 26),
                                      'ICU patient', 'Non-ICU patient')) %>%
  mutate(discharge_disposition_id = ifelse(discharge_disposition_id %in% c(13, 14, 19, 20, 21),
                                           'ICU patient', 'Non-ICU patient')) %>%
  mutate(icu_or_non = ifelse(admission_source_id == discharge_disposition_id &
                             admission_type_id   == discharge_disposition_id,
                             admission_source_id, 
                             ifelse(admission_source_id == discharge_disposition_id,
                                    admission_source_id, admission_source_id))) %>%
  # Change the label system in attribute diag_1
  mutate(diag_1 = case_when(diag_1 %in% c(390:459) ~ 'Circulatory',
                            diag_1 %in% c(460:519) ~ 'Respiratory',
                            diag_1 %in% c(520:579) ~ 'Digestive',
                            diag_1 %in% c(240:279) ~ 'Diabetes',
                            diag_1 %in% c(800:999) ~ 'Injury',
                            diag_1 %in% c(710:739) ~ 'Muscoloskeletal',
                            diag_1 %in% c(580:629) ~ 'Genitourinary',
                            TRUE ~ 'Others')) %>%
  # Set all character attributes to factor
  mutate_at(vars("time_in_hospital",
                 "hba1c_res",
                 "icu_or_non"), funs(factor)) %>%
  # Introduce a log2 scale on all numeric attributes
  mutate_at(.vars = names(select_if(., is.numeric)), ~ log2(. + 0.1)) %>%
  # Remove all redundant attributes
  select(-c('encounter_id', 'patient_nbr', 'weight', 'payer_code', 'medical_specialty', 'diag_2', 'diag_3',
            'admission_source_id', 'discharge_disposition_id', 'admission_type_id', removal.names)) %>%
  relocate(time_in_hospital, .after = last_col())

# Write final dataset to datafile
write.csv(final.data, "datasets/base_datasets/normal_dataset.csv", row.names = FALSE)
```

\newpage
# Determine quality metrics relevancy
For the next part of our research, we will work on a way to predict the time spent in the hospital. Weka, a Java-based data analysis and algorithm interface, is used in this research. Its powerful interface makes it easy to work with comprehensive data modeling techniques.

Weka comes with quite a few ways to evaluate results gathered from a classifier. When predicting a class variable, Weka reports of the number of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). These are essential values for evaluating (predicted) results after running on a classifier. Knowing these four crucial numbers, we can calculate quality metrics to determine how well a classifier works on the supplied dataset. The number of quality metrics to choose from is substantial, but we only need a couple to establish whether a classifier is worth-while. We look at the speed a classification is finished, the accuracy, precision, and recall of the results, the rates of true and false positives, F1-score measurement, determine a ROC area, and construct a confusion matrix.

The speed (in seconds) at which a classifier finishes its process is of interest for determining whether a classifier is useful or not. Some classifications may be slower than others. For example, a Naïve Bayes is much quicker than performing a J48 on this dataset due to the complexity of the dataset - 70.000 instances and 44 attributes to consider - and because both classifications have different goals. Therefore, a slower classification is not necessarily worse than a faster one; it depends on what we are interested in researching, and if classifications are similar in end goals. When the latter is true, and there is a considerable difference in construction speed, we can dismiss the slower one. In most cases, other metrics may have the final verdict.

A confusion (or error) matrix represents the performance of an algorithm. It is a table with rows for the predicted instances, and the columns represent the instances of the actual class. The matrix reports on the number of FP, FN, TP, and TN. This representation allows for a more detailed analysis than a metric that specifies a certain value like accuracy. However, it can complicate when too many class variables need to be depicted. For example, our class attribute has 14 valuations and creates a confusion matrix of 196 (14x14) elements. It contains a lot of information; therefore, we introduce other quantity metrics that work more specific and are less complex.

The number of instances correctly identified as either true positive or true negative is called the accuracy. This metric gives a peak in how "true" our classification is, or in other words, how many predictions are right. It is essential to know how accurate our classification is as we want the result (prediction) to be as close to the 'true' value.  

Recall (also called sensitivity or the true positive rate) reports on the number of instances correctly identified as positive out of the total actual positives, and precision (also called positive predictive value) is the number of instances correctly identified as positives of the total instances identified as positive. Both metrics give an insight into the relevance of a classification. The recall is related to the type II error rate and gives information about the completeness. In contrast, precision is more related to the type I error rate, which we are more interested in as this is about false positives. Having a high precision means good quality, which - mostly - translates to good accuracy. Precision can be a good metric in deciding whether a classification is worth-while or not.

An F1-score is the mean of the precision and recall; it measures the effectiveness of identification done by the recall and precision metrics. It can range from 0 to 1 (perfect precision and recall). In other words, the F1-score conveys the balance between precision and recall. Since we want to balance between precision and recall, the F1-score is a perfect metric to do this with. 

The last metric we introduce is the ROC area, which can be depicted in a ROC curve. The ROC curve is created by plotting the recall (or TPR) as a function of the false-positive rate. ROC analysis arranges a way to select optimal models and to discharge suboptimal ones.

\begin{figure}[h!]
$$\text{ACC} = \frac{\text{TP} + \text{TN}}{\text{P} + \text{N}}$$
$$\text{PPV} = \frac{\text{TP}}{\text{TP}+\text{FP}}$$
$$\text{TPR} = \frac{\text{TP}}{\text{P}}$$
$$\text{F}_{1}=\frac{2\text{TP}}{2\text{TP} + \text{FP} + \text{FN}}$$
\caption{All calculation regarding ...}
\label{Nothing yet}
\end{figure}

\newpage
# Machine learning algorithm performances
We will perform a multitude of algorithms. We are interested in performing with Naïve Bayes, Simple Logistics, Nearest Neighbour (IBk), a decision tree in the form of a J48/C4.5, and a Random Tree. Besides, we also include ZeroR and OneR to measure baseline performance. Weka gives outcome for every class valuation (so in our case 1 to 14); we take the average sum of these outcomes, as it is better for comparison with other classifiers. The results are depicted in table [NUMBER].

```{r Load results retrieved from Weka}
library(kableExtra)
normal.result <- read.csv("datasets/dataset_results_weka/base/normal.csv", sep = ";")
kbl(normal.result, booktabs = T) %>%
  kable_styling(full_width = F) %>%
  column_spec(1, bold = T)
```

We now observe the results shown in table [NUMBER]. The results of all involved classifiers show a not-so-great picture; the accuracy is very low across all classifiers, ranging from a mere 16,20% for Nearest Neighbor (IBk) to a high-low result of 23,55% with Simple Logistics. This situation is not ideal as - on average - only one-fifth of the class variable is correctly predicted. This is because the class variable is positively skewed, which means low-valuations are more present than higher values. In our case, this means patients spent just a couple of days in hospital rather than, for example, more than a week.

Next up is determining which classifiers are most worthwhile regarding our research goals. As stated earlier, accuracy and precision are the most important metrics. The best option seems to be Simple Logistics with an accuracy of 23.60% and a precision of 0,523 - both are best-performing in their category. Deciding on a runner-up appears more challenging as J48 and Naïve Bayes both perform well. Naïve Bayes scores better in every metrics we present, but J48/C4.5 has better optimization options while Naïve Bayes' result can be considered to be its final product. In addition, the accuracy difference is a mere `r 20.76 - 19.55`%, not a significant difference. For this reason, we include both J.48/C4.5 in coming sections.

\newpage
## Optimizing our classifiers
To determine the best possible algorithm, we need to play with some parameter settings of our chosen algorithms. To accomplish this, we want to use Weka's Experimenter mode. This mode allows us to explore multiple parameter settings and rule which are the most optimal. In other words, we want to crick up the accuracy of all algorithms involved. We will do this by exploring different meta-learners. The meta-learner CvParameterSelection was performed on J48/C4.5 and Simple Logistics to find the optimal parameter sets. Naïve Bayes was optimized manually, as it is a relatively simple algorithm with just a few settings. We present the results in the sections below. Just as in earlier section, we perform all the classifiers on a cost-sensitive matrix and split the test set in ten pieces with cross-validation.

### J.48/C4.5
First, we look at the parameter settings of the tree algorithm J48/C4.5. Here we have to deal with two parameters: the confidence factor parameter (C) and the minimal number of objects (M). The C value determines the amount of pruning performed; a small value corresponds to heavy pruning, whereas a large one to little pruning. The default is set to 0.25, and we will let the algorithm test with values ranging from 0.1 up to 0.5 in steps of 10. We set the limit to 0.5 as above it will cause little to no pruning. \cite{Pruning-J48} Parameter M's default is set to 2, which means that the minimum instances per leaf guarantees that at each split, at least 2 of the branches will have the minimum number of instances. Increasing this values means that leaves become bigger. We test with a range from 2 to 10 in steps of 1. The results are shown in table [NUMBER].

```{r Results J48 optimalization}
j48.results <- read.csv("datasets/dataset_results_weka/optimalization/j48-normal.csv", sep = ';')
kbl(j48.results, booktabs = T) %>%
  kable_styling(full_width = F) %>%
  column_spec(c(1, 3, 5), bold = T)
```
Looking at table [NUMBER], we observe three columns of settings that caused a given accuracy, depicted besides every used setting, respectively. We first looked at the outcomes of individual parameter setting. Thereafter we combined a few of these settings to find the most optimum parameter settings. The confidence factor shows a pattern where lower values give higher accuracy; it means that we perform more pruning (reducing the size of the decision tree) which leaves us with better outcomes. But we have to be careful with this parameter as it deems to overfit our model on the training data we provide. Therefore, we want to carefully consider this value. As for parameter M, we see an opposite pattern happening, increasing this value comes with better accuracy. With the default setting (a value of 2) returning 19.55% accuracy, we see an increase of `r 20.88 - 19.55`% percentage points as we set this to 5. With a value of 10 the accuracy becomes its most optimal point yet: 21.80%. In combining both most optimal settings we get an accuracy of 23.28%, also the highest in the Combination column. Optimizing the settings of this algorithm, we see an increase of `r 23.28 - 19.55`% at the finish line. Truthfully, it is not a significant increase from its baseline performance but it is still a better result. Considering we did performed cross-validation on the confidence factor and the outcome looks promising to not be overfitted, we can take these settings and conclude that this is the most optimum form of J48 regarding our research.

\newpage
### Simple Logistics
Now we turn to Simple Logistics (SL). SL has the advantage of built-in attribute selection. This means that it stop adding SimpleLinearRegression models when the cross-validation classification error no longer decreases. \cite{SL-model} This algorithm has quite a few parameters to its disposal, but we will not refrain ourselves to checkout every one of them, as not each one is relevant. Here we deal with the following set of parameters: useAIC (A), heuristicStop (H), maxBoostingIterations (M), numBoostingIterations (I), and weightTrimBeta (W). A determines when to stop iterations of adding models. This is an alternative to cross-validation. H is a value that decides whether a run is stopped if no new error minimum has not been reached in the last iteration of H. Weka advices to use the default value, but we are interested in its effects when we change this value. M sets a maximum number of iterations for LogiBoost. Default is set to 500, and it is adviced to use a higer number in bigger datasets, which is the case for us. Moving on to I: it is a counterpart of M in which sets a fix number of iterations for LogiBoost. If < 0, the number is cross-validated or a stopping criterion on the training set is used, which is the default. And lastly, W states that only instances carrying (1 - W)% of the weight from the previous iteration are made available for the next one - resulting in less instances but may be a more accurate model. We depict our findings in table [NUMBER].

```{r Results SL optimalization}
sl.results <- read.csv("datasets/dataset_results_weka/optimalization/sl-normal.csv", sep = ';')
kbl(sl.results, booktabs = T) %>%
  kable_styling(full_width = F) %>%
  column_spec(1, bold = T)
```

All parameters involved do not have a big impact on improving the accuracy of the classifier. Combining parameters did not help either, but their results are not shown. This proves that the SL algorithm already delivered the closest to the most optimal situation possible. As we look back to table [NUMBER], we already saw that this algorithm had a relatively high accuracy with default settings in comparison with others. In addition, we already stated that this algorithm has the ability to stop adding models when cross-validation errors no longer occur; in other words, it stops when it finds the most optimum situation. For these reasons, we do not adjust parameters and continue with the default settings.

\newpage
### Naïve Bayes
As discussed earlier, Naïve Bayes is a relatively simple algorithm with just a few parameter settings and therefore we optimized it manually. We had two options to research and present our findings in table [NUMBER], depicted underneath. These options are: 1) using kernel density estimator rather than normal distribution (K); and 2) using supervised discretization (D). The K option is a way to estimate the probability density function of a variable; in a sense it is smoothing out the data a bit, in a different way than via the normal distribution. Or in other words, it weighs observations differently depending on how far way from a certain point that we are evaluating. The D parameter considers the class value, whereas the default, unsupervised discretization, does not. Therefore, we expect a better results as the filter will break our attributes into bins that provide the most information about the class variable. We cannot combine the two parameters as that would create conflict.

```{r Results Bayes optimization}
bayes.results <- read.csv("datasets/dataset_results_weka/optimalization/bayes-normal.csv", sep = ';')
kbl(bayes.results, booktabs = T) %>%
  kable_styling(full_width = F) %>%
  column_spec(1, bold = T)
```

We now observe table [NUMBER] where both introduced parameters have a positive impact on the accuracy and all other metrics involved. Namely the K parameter increases the accuracy a few percentage points, so does the M parameter but to a less extend. In other areas the results are even closer between both parameters: 0.203 precision for K and a score of 0.199 in regard to M. But looking at the bigger picture, we conclude that K is the better option to go along with. Still, now optimized, Naïve Bayes still scores less on every metrics than SL performed in its baseline. We will discuss which of the three classifiers is the final one after some more optimization. 

\newpage
## Selecting attributes
Weka contains a meta-learner that is called 'AttributeSelectedClassifier'. This meta learner gives us the opportunity to evaluate if a given classifier performs better without certain attributes. It does this in two steps: 1) dimensionality reduction through attribute selection and; 2) passing on to a given classifier. \cite{Weka-AttribSelectClass} We have the option to rank attributes hinged on their performance in regard to the accuracy or any other metric. We are going to look at three different evaluators of this classifier to research if we are better off without any of our remaining attributes. These three different ones are 1) CorrelationAttributeEval, 2) InfoGainAttributeEval, and 3) GainRatioAttributeEval. Before dropping any attribute, we investigate the influence of a low-performing attribute. We also take their performance over the broad of evaluators we are going to use into account. This can vary between evaluators and is essential to investigate.

### Correlation
We look at the CorrelationAttributeEval which calculates the correlation between each attribute and the class variable. When we run an evaluater we look for values that outline a moderate-to-high or negative correlation, so ranging from 1 to -1. We want to avoid attributes that have a near-zero or zero correlation, as these attributes can potentially harm our model's performance. Therefore, we want to drop these attributes. The Ranker search method will be used here.

```{r Results correlation attribute evaluation}
corr.results <- read.csv("datasets/dataset_results_weka/attributes/correlation-normal.csv", sep = ";")
kbl(corr.results, booktabs = T) %>%
  kable_styling(full_width = F) %>%
  column_spec(1, bold = T)
```
We observe the results in table [NUMBER], and it is not great to say the least. Our highest scoring attribute is num_mediations with a score of 0.12651, and our least-scoring is rosiglitazone with a score of a mere 0.000389. In most experiments, it is usual to use a cut-off score of 0.05 for relevancy of an attribute. If we would do this, we are left with four of the 23 discussed attributes. That would not be worth it if we want to continue with these research goals. If we proposed another cut-off of, for example, 0.01 we would lose eight attributes instead of 19. As of now, we like to look at the results of other evaluators before making a final decision.

### Info Gain
Next up is an interesting evaluator called InfoGainAttributeEval, which looks at the information gain or entropy for each attribute for the output/class variable. Calculated values can range between 0 (no information) and 1 (maximum information). Attributes with a high valuation contribute more to the model than lower values do, as they reduce entropy more. There is the potential of removing attributes with a low valuation that could increase accuracy. Like we did in the correlation evaluator, we use the Ranker search method once more. We depict our results and findings in table [NUMBER].

```{r Results info gain attribute evaluation}
info.results <- read.csv("datasets/dataset_results_weka/attributes/info_gain-normal.csv", sep = ";")
kbl(info.results, booktabs = T) %>%
  kable_styling(full_width = F) %>%
  column_spec(1, bold = T)
```
As we have seen in the section about correlation, we see that the same set of attributes performing worse than others. Some even perform so bad that they get a score of zero - number_emergency and number_outpatient - which shows that at least these two attributes are up for removal. Deciding on a cut-off value is difficult; what is considered a 'normal' cut-off value means, again, that we would throw away at least 80% of the attributes. In the case of 0.05, we would be left with only two attributes and with 0.01 eight attributes. What we can do is look at the effects of removing sets of attributes at a time and run our classifiers again to determine the effects.

### Gain Ratio
This evaluator looks a lot like the one we just discussed. It is an enhancement of the InfoGain evaluator, with a normalized score. In more detail, this method measures the significance of attributes with respect to the class variable on the basis of gain ratio. A higher gain ratio means it reduces more entropy. We again use the Ranker search method. The results are shown in table [NUMBER].

```{r Results gain ratio attribute evaluation}
gain.results <- read.csv("datasets/dataset_results_weka/attributes/gain_ratio-normal.csv", sep = ";")
kbl(gain.results, booktabs = T) %>%
  kable_styling(full_width = F) %>%
  column_spec(1, bold = T)
```
Again, we see an occurring trend: the same five attributes - num_medications, num_lab_procedures, num_diagnosis, num_procedures and change - perform the best with each evaluator. The same can be said about the other side. The attributes number_emergency and number_outpatient are both worst performing once more. We will remove these attributes at least. In the next section, we will discuss the attributes we are going to remove and which ones are here to stay.

### Final decision
As already stated, we will remove the attributes number_emergency and number_outpatient. Considering if we needed to remove more attributes is a more difficult decision; almost all involved attributes do not score sufficient enough to even come close to the considered normal cut-off value of 0.05 or, in some cases, 0.01. Therefore, we want to explore another option that involves relevancy in the information an attribute gives us. For example, the attribute gender contains critical information - is the patient a male or a woman - regarding further research goals and being able to separate cases on gender is very important in medical studies, but has a low score in every attribute evaluation. Even though it has a low score, we want to retain it because of its content. Another example is metformin, which also scores low but is not of importance as diabetesMed. When applying this logic across the broad of attributes, we come with a selection of candidates to remove. These are, in random order: metformin, diabetesMed, rosiglitazone, pioglitazone, glipizide, and glyburide. This leaves us with 16 attributes, including the class variable. We performed all our classifiers again on the new state of the dataset. We only consider the accuracy. The results are depicted in table [NUMBER].

```{r Final results}
final.results <- read.csv("datasets/dataset_results_weka/attributes/final-attributes.csv", sep = ";")
kbl(final.results, booktabs = T) %>%
  kable_styling(full_width = F) %>%
  column_spec(1, bold = T)
```
We now observe table [NUMBER] and we can see that removing said attributes improves only the accuracy and other metrics of Naïve Bayes and J48/C4.5; Simple Logistics performs slightly worse with less attributes. Taking the best-performing version of a classifier seems to best option. So in the case of taking Simple Logistics as our final model, we need to make a decision whether we want to keep the attributes we removed that concluded in better accuracy for both other models or also get ride of them here. The attributes that were removed contained very little important information relative to others. Never mind that they resulted in getting a better accuracy in the case of Simple Logistics - but not with a big margin (just 0.13%). Therefore, we are going to continue with less attributes for each classifier. 

## Confusion matrices
In this section, we look at the confusion matrices of the best-fitting versions of our chosen classifiers. Since we have a class variable with 14 values, a matrix would have a 14x14 structure. For the reason of that complexity, we have always looked at the weighted average when comparing models. Now that we want to make a final decision on which model we are going to use, with the fact that all our models' performances are relatively close to one another, it can help to look at the details for making a final decision.

### Naïve Bayes
We will look at the outcome regarding Naïve Bayes first. The confusion matrix is shown in table [NUMBER]. When analyzing a confusion matrix it is essential know that the (longest) diagonal line represents all true predictions. Off-diagonal elements are the ones that are mislabeled by a classifier. Naturally, the higher the diagonal values, the better as it indicates many correct predictions (i.e., a high TP rate). Naïve Bayes performed relatively well with an accuracy of 23.42%.

\begin{table}[ht]
\centering
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c|c|c}
  \hline
 a    & b    & c    & d     & e   & f   & g   & h   & i   & j  & k  & l & m  & n  & <-- classified as \\ \hline
 5036 & 2811 & 1968 &  450  & 153 &  46 &  21 &  11 &   3 &  2 &  1 & 0 &  0 &  0 & | a = 1           \\
 3523 & 4023 & 3037 &  931  & 426 & 149 &  50 &  41 &  33 &  4 &  1 & 0 &  2 &  1 & | b = 2           \\
 2334 & 3239 & 4250 & 1468  & 730 & 263 &  95 & 111 &  49 &  9 &  4 & 0 &  2 &  1 & | c = 3           \\
 1292 & 2050 & 3159 & 1416  & 818 & 261 & 160 & 233 &  63 & 12 &  8 & 0 &  3 &  1 & | d = 4           \\
  723 & 1138 & 2054 & 1118  & 815 & 314 & 176 & 287 &  76 & 22 & 13 & 0 & 10 &  2 & | e = 5           \\
  444 &  719 & 1510 &  825  & 635 & 330 & 186 & 294 & 103 & 27 & 13 & 1 &  6 &  3 & | f = 6           \\
  273 &  476 & 1035 &  669  & 523 & 308 & 171 & 321 &  87 & 30 & 15 & 0 & 22 &  7 & | g = 7           \\
  208 &  260 &  694 &  427  & 392 & 235 & 148 & 298 &  98 & 59 & 28 & 2 & 16 &  7 & | h = 8           \\
   95 &  155 &  429 &  281  & 255 & 177 & 135 & 248 &  80 & 41 & 22 & 2 & 12 &  9 & | i = 9           \\
   60 &  109 &  324 &  207  & 184 & 145 & 109 & 224 &  73 & 41 & 30 & 1 & 10 &  6 & | j = 10          \\
   46 &   81 &  241 &  170  & 155 & 134 &  80 & 170 &  53 & 42 & 20 & 0 & 13 & 14 & | k = 11          \\
   39 &   54 &  182 &  123  & 104 &  94 &  64 & 139 &  58 & 34 & 21 & 2 &  6 &  6 & | l = 12          \\
   40 &   36 &  127 &   85  &  96 &  70 &  67 & 138 &  50 & 24 & 24 & 1 & 11 &  2 & | m = 13          \\
   24 &   31 &  122 &   66  &  69 &  61 &  46 & 106 &  49 & 33 & 26 & 2 & 12 &  4 & | n = 14          \\ \hline
\end{tabular}
\end{table}

When we observe the results, we see - again - that our data is positively skewed as the total instances decrease while the time in hospital spent increases. This is exactly the problem when predicting the exact time a patient is going to spent in the hospital; we have a lot of data regarding short visits and not so much with longer visits. Tackling this problem means more instances for longer visits, as we get more information regarding these instances. Something that we lack right now. In addition, the accuracy and precision per class variable also decreases with increasing total time spent in the hospital. For example, we only have four correct predictions when looking at 14 days spent in hospital, which is ... % of the total instances. Compare that to ... % for one day in hospital (a). 

### Simple Logistics
Next up is the confusion matrix retrieved from the Simple Logistics classifier, depicted in table [NUMBER]. Simple Logistics had a better accuracy without removing any initial attributes, but we decided to use the one with fewer attributes regardless. This was that supplying unnecessary attributes for a small increase in accuracy is not worth the burden. Therefore, we have a Simple Logistics classifier that performs with a 23.47% accuracy.

\begin{table}[ht]
\centering
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c|c|c}
  \hline
 a    & b    & c    & d    & e   & f   & g   & h   & i  & j  & k  & l & m  & n & <-- classified as \\ \hline
 4556 & 3091 & 2566 &  217 &  53 &   7 &  11 &   1 &  0 &  0 &  0 & 0 &  0 & 0 &   | a = 1         \\
 3127 & 4167 & 4099 &  579 & 163 &  45 &  32 &   8 &  0 &  1 &  0 & 0 &  0 & 0 &   | b = 2         \\
 2029 & 3345 & 5645 & 1027 & 325 &  93 &  70 &  20 &  1 &  0 &  0 & 0 &  0 & 0 &   | c = 3         \\
 1150 & 2092 & 4375 & 1098 & 446 & 150 & 101 &  55 &  4 &  4 &  0 & 0 &  1 & 0 &   | d = 4         \\
  646 & 1214 & 2920 & 1004 & 531 & 149 & 179 &  70 & 18 &  7 &  5 & 0 &  4 & 1 &   | e = 5         \\
  432 &  735 & 2087 &  848 & 492 & 175 & 180 & 103 & 24 & 11 &  5 & 0 &  2 & 2 &   | f = 6         \\
  256 &  518 & 1478 &  681 & 422 & 202 & 190 & 124 & 31 & 23 &  9 & 0 &  2 & 1 &   | g = 7         \\
  203 &  316 &  955 &  479 & 341 & 157 & 233 & 115 & 35 & 30 &  5 & 0 &  2 & 1 &   | h = 8         \\
   92 &  163 &  606 &  330 & 261 & 146 & 197 &  96 & 27 & 14 &  4 & 0 &  3 & 2 &   | i = 9         \\
   60 &  125 &  454 &  233 & 218 & 111 & 181 &  74 & 36 & 16 &  8 & 0 &  3 & 4 &   | j = 10        \\
   39 &   86 &  361 &  181 & 181 & 102 & 129 &  72 & 28 & 19 &  8 & 0 & 11 & 2 &   | k = 11        \\
   34 &   57 &  255 &  142 & 133 &  78 & 115 &  58 & 24 & 16 &  5 & 0 &  5 & 4 &   | l = 12        \\
   34 &   43 &  197 &   77 & 122 &  58 & 119 &  54 & 31 & 14 & 12 & 1 &  6 & 3 &   | m = 13        \\
   18 &   35 &  167 &   79 &  85 &  67 &  91 &  43 & 29 & 21 &  6 & 0 &  9 & 1 &   | n = 14        \\ \hline
\end{tabular}
\end{table}

The presented table looks quite familiar to Naïve Bayes - positively skewed and not many predictions regarding higher valuations. We can see but is a little difficult to do so, that the number of correct predictions regarding a is significantly less than seen in Naïve Bayes. The numbers stand at 4.556 for Simple Logistics and 5.036 regarding Naïve Bayes. In contrast, the true positives of other class variables are higher across the board... but this effect wears off when the class variables increase. This effect could be the result of the weight Simple Logistics puts on attributes. It is a more complex classifier than Naïve Bayes. However, it does not result in a better-performing classifier.

### J48/C4.5
Last but not least is J48/C4.5, where accuracy of 23.42% was achieved after attribute selection. The confusion matrix of the classifier is shown in table [NUMBER].

\begin{table}[ht]
\centering
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c|c|c}
  \hline
 a    & b    & c    & d   & e   & f   & g   & h   & i  & j  & k  & l  & m  & n  & <-- classified as \\ \hline
 5071 & 2593 & 2258 & 315 & 122 &  72 &  39 &  18 &  3 &  2 &  4 &  2 &  1 &  2 &   | a = 1         \\
 3344 & 3746 & 3973 & 570 & 275 & 147 &  86 &  35 & 19 &  9 & 13 &  2 &  2 &  0 &   | b = 2         \\
 2137 & 3095 & 5346 & 922 & 481 & 256 & 159 &  71 & 38 & 20 & 19 &  4 &  4 &  3 &   | c = 3         \\
 1257 & 1906 & 4193 & 882 & 541 & 279 & 204 & 103 & 42 & 26 & 19 & 10 &  8 &  6 &   | d = 4         \\
  643 & 1181 & 2787 & 697 & 517 & 367 & 244 & 136 & 60 & 52 & 33 &  9 & 11 & 11 &   | e = 5         \\
  440 &  730 & 1968 & 573 & 444 & 352 & 269 & 134 & 64 & 42 & 36 & 16 & 16 & 12 &   | f = 6         \\
  284 &  508 & 1417 & 438 & 395 & 282 & 230 & 169 & 79 & 58 & 43 & 12 & 14 &  8 &   | g = 7         \\
  186 &  358 &  884 & 301 & 304 & 228 & 216 & 181 & 56 & 63 & 55 & 12 & 22 &  6 &   | h = 8         \\
   95 &  191 &  550 & 199 & 205 & 190 & 182 & 132 & 59 & 47 & 45 & 10 & 22 & 14 &   | i = 9         \\
   71 &  138 &  402 & 174 & 169 & 129 & 130 & 126 & 38 & 40 & 48 & 25 & 21 & 12 &   | j = 10        \\ 
   55 &   92 &  294 & 120 & 141 & 114 & 116 & 107 & 58 & 44 & 35 & 13 & 17 & 13 &   | k = 11        \\
   36 &   73 &  222 &  99 & 107 &  93 &  84 &  80 & 35 & 31 & 33 & 11 & 13 &  9 &   | l = 12        \\
   37 &   49 &  178 &  74 &  73 &  64 &  84 &  78 & 33 & 39 & 29 & 11 & 13 &  9 &   | m = 13        \\
   27 &   46 &  143 &  59 &  59 &  77 &  44 &  69 & 26 & 35 & 36 &  6 & 13 & 11 &   | n = 14        \\ \hline
\end{tabular}
\end{table}

Again, we see almost the same results in the performance. Some differences between individual class variables' performances, but not many dissimilarities compared with the other discussed classifiers. Another point regarding J48 is that the tree produced is massive and too complex to be informative. This is also related to the number of attributes and class variables. The reason given can be legitimate to renounce this classifier because of the great similarities between all involved classifiers.

Observing the chosen classifiers' confusion matrices did not result in a breakthrough in choosing the final model. As we already established before discussing the matrices, is that the classifiers perform almost perform the same. There are some differences in the accuracy per class variable, but this is not enough overall. Having an accuracy of around 23% is just not acceptable for a model that needs to exactly predict the time spent in the hospital. This is mostly due to the skewness of our data. For example, having more instances regarding longer visits would help determine the difference between the total time spent in the hospital. A model predicting the exact length in days is too good to be true; therefore, we want to explore different scenarios where a more accurate model is helpful. And, of course, thinking about methods to increase the accuracy. This is exactly what we are going to do in the next sections.

\newpage
# Change of plans
As we have seen in the presented results in the sections above, the accuracy of our model is softly said not great. We top at an accuracy of [NUMBER] which is general seen as too low to be of use in daily life situation. In this section we explore more abrupt ways to increase the accuracy; think of restructuring or removing instances from our class variable, which we did not want to do at first.

## Effects of removing instances
```{r Calculating amount of instances per value, include = FALSE}
count.data <- final.data %>%
  group_by(time_in_hospital) %>%
  tally()

# Make a table out of our count data
kbl(count.data, booktabs = T) %>%
  kable_styling(full_width = F) %>%
  column_spec(1, bold = T)

instance.data <- final.data %>%
  filter(!time_in_hospital %in% c(11, 12, 13, 14))

write.csv(instance.data, "datasets/base_datasets/less_instances.csv", row.names = FALSE)
```

As we have seen, there is no magic trick that would increase the accuracy without severely adjusting our data. Think about removing instances with higher valuation, reconstructing instances, or manually giving weight to high values to create a more normal-looking distribution. We already performed a cost-sensitive classifier to give more weight to values with fewer instances. Awarding even more weight could create a false image of what is really going on. Removing instances with higher values that are less covered is the last thing we want to do. Then we need to draw a line somewhere: what values can stay and which ones need to go? To illustrate the effects of removing high-value instances, we consider removing instances with a valuation ranging from 11 to 14; therefore, we would remove `r count.data %>% filter(time_in_hospital %in% c("11", "12", "13", "14")) %>% numcolwise(sum)(.)` instances, which translates to a loss of around 6%. After removal, we are left with 66.871 instances. Now that we have a new, temporary dataset, we turn to Weka once more and see if the rate of correctly classified instances has improved. The short-hand answer to that question is yes, it has improved, but not substantially enough to consider removing these instances for our final model. We performed the J48 and Simple Logistics classifiers again and observed an improvement of 1,13% compared to our first run (19,81% versus 20,94%) for J48. We noticed a similar trend for Simple Logistics. Here, the improvement is just 1,33%. The results are depicted in table [NUMBER] . Removing 6% of our instances for such little improvement in accuracy overall is not worthwhile; therefore, we can safely conclude that this method is not what we are looking for in determining a better accuracy.

```{r Inserting results with less instances}
diff.instances <- read.csv("datasets/dataset_results_weka/base/less_instances.csv", sep = ";")
kbl(diff.instances, booktabs = T) %>%
  kable_styling(full_width = F) %>%
  column_spec(1, bold = T)
```
[DISCUSS]

\newpage
## A new research goal
Another option to improve accuracy could be a relabeling of the attribute. This method has the upside of retaining all instances. The downside of this method is that we create bigger data groups, thereby removing the chance of exactly determining how many days were spent in the hospital. While we cannot determine exactly how many days a patient spent in the hospital, we can separate visits based on classification, such as "a brief visit" or "a longer visit". So, classification is still possible but is less specific. This is the price to pay if we want a more accurate model. We considered a relabeling with two different valuations: 
\begin{enumerate}
  \item Time spent in hospital is within or equal to 5 days; and
  \item Time spent in hospital is longer than five days.
\end{enumerate}

In the next code section, we mutate the time_in_hospital attribute, as described above. We also show the implications of relabeling in figures ... and ... . As we can see, we now have two classes comprising of [NOT FINISHED]

```{r Creating new labels}
final.data.label <- final.data %>%
  mutate(time_in_hospital = case_when(time_in_hospital %in% c('1', '2', '3', '4', '5') ~ '1',
                                      TRUE ~ '2')) %>%
  # Added 13 of October: Reshape data so that class variable is the last column
  # We did this manually with using the Explorer, but now we use the Experimenter,
  # and the only way to appoint a class variable is having it be the last column.
  relocate(time_in_hospital, .after = last_col())

# Add legends and introduce a two-in-one plot
ggplot() +
  geom_bar(mapping = aes(x=final.data$time_in_hospital)) +
  ggtitle("Time in hospital (1-14 days)") +
  xlab("Days")

ggplot() +
  geom_bar(mapping = aes(x=final.data.label$time_in_hospital)) +
  ggtitle("Time in hospital (four classifications)") +
  xlab("Classification")
  
write.csv(final.data.label, "datasets/base_datasets/relabeled.csv", row.names = FALSE)
```

Now that we have a new dataset, we turn to Weka once more to analyze the new dataset. The results of this are shown in table [NUMBER]. Having obtained results regarding both datasets, we can determine the differences. As expected, accuracy rose across the broad of classifiers; most experienced an increase of at least [NUMBER] percentage points, except for IBk, which rose [NUMBER] percentage points. IBk already had the least sufficient outcome in previous Weka runs, so logically we would not have expected it to become best-performing. When considering taking a final dataset to the next stage in our research, we must weigh if a less specific attribute is worth having better accuracy (and for every other metric, for that matter). As we already discussed, having a sense of the duration spent in the hospital is still present. Additionally, we can now predict more accurately by a large margin. Even now, classifiers only predict correctly half of the time, and is even worse when we do not relabel. In contrast, if we come put with a way to produce more sufficient outcomes with the initial dataset, we can always switch back. It seems that the relabeled dataset looks the most promising right now, and therefore we will continue our analysis with this dataset. However, we will report on the initial dataset in the next section as well. 

```{r Results relabeling}
label.result <- read.csv("datasets/dataset_results_weka/base/relabeled.csv", sep = ";")
kbl(label.result, booktabs = T) %>%
  kable_styling(full_width = F) %>%
  column_spec(1, bold = T)
```

[DISCUSS FINDINGS]

## Optimalization
[INTRO]

### Simple Logistics
[INTRO]

```{r SL optimalization}
#sl.results <- read.csv()
```

[DISCUSS]

### Random Forest
[INTRO]

```{r RF optimalization}
#rf.results <- read.csv()
```

[DISCUSS]

## Attribute selection
[INTRODUCTION]

### Correlation
[INTRO]

```{r Correlation - attribute selection}
#corr.results <- read.csv()
```

[DISCUSS]

### Information gain
[INTRO]

```{r Information gain - attribute selection}
#infogain.results <- read.csv()
```

[DISCUSS]

### Gain ratio
[INTRO]

```{r Gain ratio - attribute selection}
#gainr.results <- read.csv()
```

[DISCUSS]

### Final decision
[INTRO]

```{r Final decision - attribute selection}

```

[DISCUSS]

## Confusion matrices
[INTRO]

[MATRIX 1]

[DISCUSS]

[MATRIX 2]

[DISCUSS]

## A ROC curve
[INTRO]
```{r ROC curve}
## TEMP
#roc_data <- read.table("~/Desktop/roctest.arff", 
#                   sep = ",", 
#                   comment.char = "@")
#names(roc_data) <- c("Instance_number", 
#                 "True_Positives",
#                 "False_Negatives",
#                 "False_Positives",
#                 "True_Negatives",
#                 "False_Positive_Rate",
#                 "True_Positive_Rate",
#                 "Precision",
#                 "Recall",
#                 "Fallout",
#                 "FMeasure",
#                 "Sample_Size",
#                 "Lift",
#                 "Threshold")

#head(roc_data)

#library(ggpubr)
#colors <- c(classifier = "red", threshold = "blue")
#plt <- ggplot(data = roc_data,
#       mapping = aes(x = True_Positive_Rate, y = False_Positive_Rate)) +
#    geom_point(mapping = aes(color = "classifier")) +
#    geom_abline(aes(color = "threshold", 
#                    slope = 1, 
#                    intercept = 0)) + 
#    scale_color_manual(values = colors) +
#    xlab("True Positive Rate") +
#    ylab("False Positive Rate") +
    #theme_minimal() +
#    theme_pubr() +
#    theme(legend.title = element_blank())
#print(plt)
```

[DISCUSS]

[FINAL WORD]

# Settings used
(normal)
#1) meta.CostSensitiveClassifier '-cost-matrix \"[0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0]\" -S 1 -W rules.ZeroR' -110658209263002404
#(2) meta.CostSensitiveClassifier '-cost-matrix \"[0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0]\" -S 1 -W rules.OneR -- -B 6' -110658209263002404
#(3) meta.CostSensitiveClassifier '-cost-matrix \"[0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0]\" -S 1 -W bayes.NaiveBayes' -110658209263002404
#(4) meta.CostSensitiveClassifier '-cost-matrix \"[0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0]\" -S 1 -W functions.SimpleLogistic -- -I 0 -M 500 -H 50 -W 0.0' -110658209263002404
#(5) meta.CostSensitiveClassifier '-cost-matrix \"[0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0]\" -S 1 -W lazy.IBk -- -K 1 -W 0 -A \"weka.core.neighboursearch.LinearNNSearch -A \\\"weka.core.EuclideanDistance -R first-last\\\"\"' -110658209263002404
#(6) meta.CostSensitiveClassifier '-cost-matrix \"[0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0]\" -S 1 -W trees.J48 -- -C 0.25 -M 2' -110658209263002404
#(7) meta.CostSensitiveClassifier '-cost-matrix \"[0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0]\" -S 1 -W trees.RandomTree -- -K 0 -M 1.0 -V 0.001 -S 1' -110658209263002404

(remove_instances)
#1) meta.CostSensitiveClassifier '-cost-matrix \"[0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0]\" -S 1 -W trees.RandomTree -- -K 0 -M 1.0 -V 0.001 -S 1' -110658209263002404
#(2) meta.CostSensitiveClassifier '-cost-matrix \"[0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0]\" -S 1 -W functions.SimpleLogistic -- -I 0 -M 500 -H 50 -W 0.0' -110658209263002404
#(3) meta.CostSensitiveClassifier '-cost-matrix \"[0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0]\" -S 1 -W bayes.NaiveBayes' -110658209263002404
#(4) meta.CostSensitiveClassifier '-cost-matrix \"[0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0]\" -S 1 -W rules.OneR -- -B 6' -110658209263002404
#(5) meta.CostSensitiveClassifier '-cost-matrix \"[0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0]\" -S 1 -W rules.ZeroR' -110658209263002404
#(6) meta.CostSensitiveClassifier '-cost-matrix \"[0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0]\" -S 1 -W lazy.IBk -- -K 1 -W 0 -A \"weka.core.neighboursearch.LinearNNSearch -A \\\"weka.core.EuclideanDistance -R first-last\\\"\"' -110658209263002404
#(7) meta.CostSensitiveClassifier '-cost-matrix \"[0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 1.0; 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0]\" -S 1 -W trees.J48 -- -C 0.25 -M 2' -110658209263002404

(relabeled)
# meta.CostSensitiveClassifier '-cost-matrix \"[0.0 1.0 1.0 1.0; 1.0 0.0 1.0 1.0; 1.0 1.0 0.0 1.0; 1.0 1.0 1.0 0.0]\" -S 1 -W rules.ZeroR
#(1) meta.CostSensitiveClassifier '-cost-matrix \"[0.0 1.0 1.0 1.0; 1.0 0.0 1.0 1.0; 1.0 1.0 0.0 1.0; 1.0 1.0 1.0 0.0]\" -S 1 -W rules.OneR -- -B 6' -110658209263002404
#(2) meta.CostSensitiveClassifier '-cost-matrix \"[0.0 1.0 1.0 1.0; 1.0 0.0 1.0 1.0; 1.0 1.0 0.0 1.0; 1.0 1.0 1.0 0.0]\" -S 1 -W trees.J48 -- -C 0.25 -M 2' -110658209263002404
#(3) meta.CostSensitiveClassifier '-cost-matrix \"[0.0 1.0 1.0 1.0; 1.0 0.0 1.0 1.0; 1.0 1.0 0.0 1.0; 1.0 1.0 1.0 0.0]\" -S 1 -W bayes.NaiveBayes' -110658209263002404
#(4) meta.CostSensitiveClassifier '-cost-matrix \"[0.0 1.0 1.0 1.0; 1.0 0.0 1.0 1.0; 1.0 1.0 0.0 1.0; 1.0 1.0 1.0 0.0]\" -S 1 -W functions.SimpleLogistic -- -I 0 -M 500 -H 50 -W 0.0' -110658209263002404
#(5) meta.CostSensitiveClassifier '-cost-matrix \"[0.0 1.0 1.0 1.0; 1.0 0.0 1.0 1.0; 1.0 1.0 0.0 1.0; 1.0 1.0 1.0 0.0]\" -S 1 -W lazy.IBk -- -K 1 -W 0 -A \"weka.core.neighboursearch.LinearNNSearch -A \\\"weka.core.EuclideanDistance -R first-last\\\"\"' -110658209263002404
#(6) meta.CostSensitiveClassifier '-cost-matrix \"[0.0 1.0 1.0 1.0; 1.0 0.0 1.0 1.0; 1.0 1.0 0.0 1.0; 1.0 1.0 1.0 0.0]\" -S 1 -W trees.RandomTree -- -K 0 -M 1.0 -V 0.001 -S 1' -110658209263002404

\newpage
# References
\begin{thebibliography}{9}
\bibitem{OrgData}
Beata Strack, Jonathan P. DeShazo, Chris Gennings, Juan L. Olmo, Sebastian Ventura, Krzysztof J. Cios, and John N. Clore, \textit{Impact of HbA1c Measurement on Hospital Readmission Rates: Analysis of 70,000 Clinical Database Patient Records}, BioMed Research International, vol. 2014, Article ID 781670, 11 pages, 2014. DOI: \url{https://doi.org/10.1155/2014/781670}
\bibitem{ICD}
Centers for Disease Control and Prevention, National Center for Health Statistics, \textit{ICD-9}, \url{https://www.cdc.gov/nchs/icd/icd9.htm}, November 6, 2015.
\bibitem{Weka-AttribSelectClass}
Hall, M, \textit{Class AttributeSelectedClassifier}, \url{https://weka.sourceforge.io/doc.dev/weka/classifiers/meta/AttributeSelectedClassifier.html}.
\bibitem{Pruning-J48}
Stiglic, G., Kocbek S., Pernek I., Kokol P., \textit{Comprehensive Decision Tree Models in Bioinformatics}. DOI: \url{10.1371/journal.pone.0033812}
\bibitem{SL-model}
Eibe Frank, Administrator; \textit{Logistic VS Simple Logistic}, \url{https://weka.8497.n7.nabble.com/Logistic-VS-Simple-Logistic-td31410.html}
\end{thebibliography}
